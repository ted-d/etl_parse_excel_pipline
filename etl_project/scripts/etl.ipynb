{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9507bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b112cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in =r\"C:\\Users\\Tony_Mony\\Desktop\\projects\\etl_project\\data\\in.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f033805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import column_index_from_string\n",
    "from map import INDICATOR_MAPPING,REGION_NORMALIZATION,DB_CONFIG\n",
    "import psycopg2\n",
    "# Ваш существующий INDICATOR_MAPPING остается без изменений\n",
    "\n",
    "def normalize_region(region_name):\n",
    "    \"\"\"Приводит название региона к стандартному виду\"\"\"\n",
    "    if not region_name or not isinstance(region_name, str):\n",
    "        return None\n",
    "    \n",
    "    # Приводим к нижнему регистру и удаляем лишние символы\n",
    "    cleaned = re.sub(r'[^\\w\\s\\-]', '', region_name.lower().strip())\n",
    "    \n",
    "    # Ищем совпадения с нашим словарем\n",
    "    for pattern, standard in REGION_NORMALIZATION.items():\n",
    "        if re.search(pattern, cleaned):\n",
    "            return standard\n",
    "    \n",
    "    # Если не нашли совпадение, возвращаем оригинал с базовой очисткой\n",
    "    return region_name.strip()\n",
    "def extract_company(text):\n",
    "    # Нормализация текста\n",
    "    text = re.sub(r'_{2,}', ' ', text)  # Заменяем множественные подчеркивания\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Заменяем множественные пробелы\n",
    "    text = text.replace('«', '\"').replace('»', '\"')  # Стандартизируем кавычки\n",
    "    \n",
    "    # Улучшенный паттерн поиска\n",
    "    pattern = r'''\n",
    "        (ПАО|АО|ООО|ЗАО|НКО)  # Тип организации\n",
    "        \\s*                    # Возможные пробелы\n",
    "        \"                      # Открывающая кавычка\n",
    "        (                      # Начинаем захват названия\n",
    "          (?:                  # Группа без захвата\n",
    "            [^\"\\n]+            # Любые символы кроме кавычки и переноса строки\n",
    "            (?:\"[^\"\\n]+)*      # Вложенные кавычки с текстом\n",
    "          )                    #\n",
    "        )                      # Конец названия\n",
    "        \"                      # Закрывающая кавычка\n",
    "        (?!\\S)                 # Не должно быть букв/цифр после\n",
    "    '''\n",
    "    \n",
    "    match = re.search(pattern, text, re.VERBOSE | re.IGNORECASE)\n",
    "    if match:\n",
    "        company_type = match.group(1).upper()\n",
    "        company_name = match.group(2)\n",
    "        \n",
    "        # Очистка названия (без обрезания существенной части)\n",
    "        clean_name = company_name.strip(' _-')\n",
    "        return f'{company_type} \"{clean_name}\"'\n",
    "    \n",
    "    # Дополнительный поиск для случаев с большим количеством пробелов\n",
    "    alt_pattern = r'(ПАО|АО|ООО|ЗАО|НКО)\\s+\"([^\"]+)\"'\n",
    "    alt_match = re.search(alt_pattern, text, re.IGNORECASE)\n",
    "    if alt_match:\n",
    "        return f'{alt_match.group(1).upper()} \"{alt_match.group(2).strip()}\"'\n",
    "    \n",
    "    return None\n",
    "def cut_company_name(company):\n",
    "    company = company.split()\n",
    "    if len(company)>=3:\n",
    "        company=company[:3]\n",
    "    return \" \".join(company)\n",
    "\n",
    "def extract_metadata(sheet):\n",
    "    \"\"\"Извлекает метаданные из листа\"\"\"\n",
    "    metadata = {\n",
    "        'subject':None,\n",
    "        'company': None,\n",
    "        'license': None,\n",
    "        'area': None,\n",
    "        'vink': None\n",
    "    }\n",
    "    \n",
    "    # Поиск названия компании\n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        if not any(row):\n",
    "            continue\n",
    "        \n",
    "        row_text = ' '.join(str(cell) for cell in row if cell).replace('\\n', ' ')\n",
    "        \n",
    "        if \"(наименование компании)\" in row_text or \"выполненных в\" in row_text :\n",
    "            match = extract_company(row_text)\n",
    "            if match:\n",
    "                metadata['company'] = cut_company_name(match)\n",
    "            break\n",
    "    for row in sheet.iter_rows(max_row=15, values_only=True):\n",
    "        if not any(row):\n",
    "            continue\n",
    "            \n",
    "        row_text = ' '.join(str(cell) for cell in row if cell)\n",
    "        if not row_text:\n",
    "            continue\n",
    "            \n",
    "        # Проверяем типичные упоминания региона\n",
    "        if any(word in row_text.lower() for word in ['хмао', 'ханты', 'югра', 'ямал', 'янао']):\n",
    "            metadata['subject'] = normalize_region(row_text)\n",
    "            break\n",
    "    # Поиск лицензии и ВИНК\n",
    "    license_pattern = re.compile(r'Лицензия\\s*([А-Я]{2,}\\s*\\d+\\s*[А-Я]{2,})[\\s,]*([А-Яа-я\\-\\s]+)')\n",
    "    vink_pattern = re.compile(\n",
    "    r'ВИНК\\*\\s*((?:ПАО|АО|ООО|ЗАО|НКО)\\s*)?'  # Орг. форма (опционально, сохраняем)\n",
    "    r'(\"[^\"]*(?:\"[^\"]*)*\"|[^\"\\s]+)'  # Название (с кавычками или без)\n",
    "    r'(?=\\s*(?:доля|$|\\n))',  # Стоп-символы\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "      \n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        if not any(row):\n",
    "            continue\n",
    "        row_text = ' '.join(str(cell) for cell in row if cell)\n",
    "        \n",
    "        # Поиск лицензии\n",
    "        license_match = license_pattern.search(row_text)\n",
    "        if license_match and not metadata['license']:\n",
    "            metadata['license'] = license_match.group(1).strip()\n",
    "            metadata['area'] = license_match.group(2).strip().replace(\" Шельфовое продолжение\",'')\n",
    "        \n",
    "        # Поиск ВИНК\n",
    "        vink_match = vink_pattern.search(row_text)\n",
    "        if vink_match:\n",
    "            # Объединяем все непустые группы через пробел\n",
    "            combined = ' '.join(group for group in vink_match.groups() if group)\n",
    "            metadata['vink'] = combined\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def process_workbook(file_path):\n",
    "    wb = load_workbook(file_path,data_only=True)\n",
    "    all_data = []\n",
    "    i=1\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT MAX(id) FROM oil_gas_data;\")\n",
    "    \n",
    "        max_id = cur.fetchone()\n",
    "        i=max_id[0]+1\n",
    "    \n",
    "    except:\n",
    "        print(\"таблица не создана\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    for sheet_name in wb.sheetnames:\n",
    "        sheet = wb[sheet_name]\n",
    "        \n",
    "        # Извлекаем метаданные один раз для листа\n",
    "        metadata = extract_metadata(sheet)\n",
    "        \n",
    "        # Находим колонки с данными\n",
    "        fact_col = plan_col = None\n",
    "        for row in sheet.iter_rows(max_row=10):\n",
    "            for cell in row:\n",
    "                if cell.value and isinstance(cell.value, str):\n",
    "                    text = str(cell.value).lower()\n",
    "                    if \"2022\" in text and \"факт\" in text:\n",
    "                        fact_col = cell.column_letter\n",
    "                    elif \"2023\" in text and \"план\" in text:\n",
    "                        plan_col = cell.column_letter\n",
    "        \n",
    "        if not fact_col or not plan_col:\n",
    "            print(f\"Не найдены колонки с данными в листе: {sheet_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Сначала собираем ВСЕ показатели за 2022 год\n",
    "        records_2022 = {\n",
    "            'ID':i,\n",
    "            'Субъект РФ': metadata.get(\"subject\"),\n",
    "            'Период (год)': 2022,\n",
    "            'план/факт':'факт',\n",
    "            'Главная компания': metadata.get('company'),\n",
    "            'Недропользователь': metadata.get('vink'),\n",
    "            '№ лицензии': metadata.get('license'),\n",
    "            'участок': metadata.get('area'),\n",
    "            'Оценка выполнения лицензионных условий': None\n",
    "        }\n",
    "        i+=1\n",
    "        \n",
    "        # Затем ВСЕ показатели за 2023 год\n",
    "        records_2023 = {\n",
    "            'ID':i,\n",
    "            'Субъект РФ': metadata.get(\"subject\"),\n",
    "            'Период (год)': 2023,\n",
    "            'план/факт': 'план',\n",
    "            'Главная компания': metadata.get('company'),\n",
    "            'Недропользователь': metadata.get('vink'),\n",
    "            '№ лицензии': metadata.get('license'),\n",
    "            'участок': metadata.get('area'),\n",
    "            'Оценка выполнения лицензионных условий': None\n",
    "        }\n",
    "        i+=1\n",
    "        # Заполняем показатели\n",
    "        for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "            if not row or not isinstance(row[0], str):\n",
    "                continue\n",
    "                \n",
    "            p_p = row[0].strip()\n",
    "            if p_p in INDICATOR_MAPPING:\n",
    "                col_name = INDICATOR_MAPPING[p_p]\n",
    "                records_2022[col_name] = row[column_index_from_string(fact_col)-1]\n",
    "                records_2023[col_name] = row[column_index_from_string(plan_col)-1]\n",
    "        \n",
    "        # Добавляем в общий список\n",
    "        all_data.append(records_2022)\n",
    "        all_data.append(records_2023)\n",
    "    \n",
    "    # Создаем DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Заполняем None для отсутствующих показателей\n",
    "    for col in INDICATOR_MAPPING.values():\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    df.iloc[:, 8] = df.iloc[:, 72]\n",
    "\n",
    "# Удаление колонки 'BU'\n",
    "    df.drop(df.columns[72], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Сохранение в Excel и CSV\n",
    "def save_results(df, base_path):\n",
    "    import os\n",
    "    # Сохранение в Excel\n",
    "    excel_path = f\"{base_path}/результат.xlsx\"\n",
    "    df.to_excel(excel_path, index=False)\n",
    "    \n",
    "    # Сохранение в CSV\n",
    "    csv_path = f\"{base_path}/результат.csv\"\n",
    "      # Сохранение в CSV с накоплением\n",
    "    if os.path.exists(csv_path):\n",
    "        try:\n",
    "            # Читаем существующие данные\n",
    "            existing_csv = pd.read_csv(csv_path)\n",
    "            # Объединяем со старыми данными\n",
    "            combined_csv = pd.concat([existing_csv, df], ignore_index=True)\n",
    "            # Удаляем дубликаты (если нужно)\n",
    "            combined_csv.drop_duplicates(inplace=True)\n",
    "            # Сохраняем\n",
    "            combined_csv.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при добавлении в CSV: {e}\")\n",
    "            # Если не получилось добавить - сохраняем новый файл\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    else:\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"Данные успешно сохранены в {excel_path} и {csv_path}\")\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175ef2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно сохранены в C:\\Users\\Tony_Mony\\Desktop\\projects\\etl_project\\results/результат.xlsx и C:\\Users\\Tony_Mony\\Desktop\\projects\\etl_project\\results/результат.csv\n"
     ]
    }
   ],
   "source": [
    "#запуск ф-ий\n",
    "out_path =r'C:\\Users\\Tony_Mony\\Desktop\\projects\\etl_project\\results' \n",
    "result_df = process_workbook(path_in)\n",
    "save_results(result_df,out_path)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c7b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from map import SHORT_NAMES,DB_COL,DB_CONFIG\n",
    "\n",
    "def generate_create_table_sql():\n",
    "    \"\"\"Генерирует SQL для создания таблицы с короткими именами колонок\"\"\"\n",
    "    # Базовые колонки\n",
    "    base_columns = [\n",
    "        'id SERIAL PRIMARY KEY',\n",
    "        'subject VARCHAR(100)',\n",
    "        'company VARCHAR(200)',\n",
    "        'license_number VARCHAR(50)',\n",
    "        'area VARCHAR(200)',\n",
    "        'vink VARCHAR(200)',\n",
    "        'year INTEGER',\n",
    "        'plan_fact VARCHAR(10)',\n",
    "        'license_evaluation FLOAT'\n",
    "    ]\n",
    "    \n",
    "    # Создаем словарь для маппинга оригинальных названий в короткие\n",
    "\n",
    "    \n",
    "    # Добавляем колонки индикаторов\n",
    "    indicator_columns = [f\"{SHORT_NAMES[key]} FLOAT\" for key in INDICATOR_MAPPING.keys()]\n",
    "    \n",
    "    return f\"CREATE TABLE IF NOT EXISTS oil_gas_data ({', '.join(base_columns + indicator_columns)})\"\n",
    "\n",
    "def load_to_postgres(df):\n",
    "    \"\"\"Загружает данные в PostgreSQL с короткими именами колонок\"\"\"\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Создаем таблицу\n",
    "    try:\n",
    "        cur.execute(generate_create_table_sql())\n",
    "        \n",
    "        # Полный маппинг оригинальных названий в короткие\n",
    "        column_mapping = {\n",
    "            # Базовые колонки\n",
    "            'ID': 'id',\n",
    "            'Субъект РФ': 'subject',\n",
    "            'Главная компания': 'company',\n",
    "            'Недропользователь': 'vink',\n",
    "            '№ лицензии': 'license_number',\n",
    "            'участок': 'area',\n",
    "            'Период (год)': 'year',\n",
    "            'план/факт': 'plan_fact',\n",
    "            'Оценка выполнения лицензионных условий': 'license_evaluation',\n",
    "            \n",
    "            # Индикаторы - автоматически генерируем на основе INDICATOR_MAPPING\n",
    "            **{v: k for k, v in DB_COL.items()}\n",
    "        }\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        # Подготавливаем SQL для вставки (только существующие колонки)\n",
    "        available_columns = [col for col in df.columns if col in column_mapping]\n",
    "        placeholders = ', '.join(['%s'] * len(available_columns))\n",
    "        sql = f\"INSERT INTO oil_gas_data ({', '.join([column_mapping[col] for col in available_columns])}) VALUES ({placeholders})\"\n",
    "        \n",
    "        # Загружаем данные построчно\n",
    "        for _, row in df.iterrows():\n",
    "            values = [row[col] if pd.notna(row[col]) else None for col in available_columns]\n",
    "            cur.execute(sql, values)\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"Успешно загружено {len(df)} записей\")\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except:\n",
    "        cur.close()\n",
    "        conn.close() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e636356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Успешно загружено 20 записей\n"
     ]
    }
   ],
   "source": [
    "load_to_postgres(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d9bcbc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
